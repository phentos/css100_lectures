{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd4307c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to `nltk` in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a695b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture goals\n",
    "\n",
    "- Introducing the `nltk` library in Python.  \n",
    "- Basic text processing with `nltk`.  \n",
    "   - Tokenization. \n",
    "   - Stemming and lemmatizing.  \n",
    "   - Part-of-speech tagging. \n",
    "   - Processing web text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e16632",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The `nltk` library\n",
    "\n",
    "> The **`nltk`** (\"Natural Language Toolkit\") library contains a set of functions, classes, and *corpora* (databases) to help process and analyze language data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3894d4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "### usually we'll import more specific functions or clasess\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbbfc63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `nltk`: a brief history\n",
    "\n",
    "- [`nltk`](https://en.wikipedia.org/wiki/Natural_Language_Toolkit) was created by Steven Bird and Edward Loper. \n",
    "- Has a *ton* of useful functions.  \n",
    "  - Built-in **corpora** of multiple genres and languages. \n",
    "  - Can **pre-process** (e.g., tokenize) text.\n",
    "  - Can **extract** information from text.  \n",
    "  - Includes basic **machine learning** applications. \n",
    "- Used by *linguists*, *NLP practitioners*, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6badc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "> `nltk` has a number of **tokenization** functions built in, including for tokenizing *sentences* (`sent_tokenize`) and indiviudal words (`word_tokenize`).\n",
    "\n",
    "`nltk` has some [additional documentation](https://www.nltk.org/api/nltk.tokenize.html?highlight=tokenize) here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bcf2f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Refresher: what is tokenization?\n",
    "\n",
    "> **Tokenization** is the process of splitting a chunk of text into individuals elements or \"tokens\" (often *words*). \n",
    "\n",
    "- Tokenization is critical for *pre-processing* text.  \n",
    "- Before counting, calculating sentiment, etc., must identify the tokens!\n",
    "- It's also surprisingly challenging to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c6d8cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenizing *words*\n",
    "\n",
    "- Before, we used `split` to tokenize a string into words.  \n",
    "- `nltk` has a function called `word_tokenize` that does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ee223f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/seantrott/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing step\n",
    "import nltk\n",
    "nltk.download('punkt') ### May need to install/download this\n",
    "\n",
    "### import word_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e127a94c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### `word_tokenize` in action (1)\n",
    "\n",
    "Here, we see that it works well on our example sentence from last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df00044",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_str = \"The quick brown fox jumped over the lazy dog\"\n",
    "word_tokenize(example_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9154d60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### `word_tokenize` in action (2)\n",
    "\n",
    "It also does a better job with the Declaration of Independence, which `split` didn't work so well with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3927d888",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "### Declaration of Independence\n",
    "with open(\"data/text/declaration.txt\", \"r\") as f:\n",
    "    contents = f.read()\n",
    "\n",
    "tokens = word_tokenize(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9084a5b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In', 'Congress', ',', 'July', '4', ',']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddb3c99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### `word_tokenize` in action (3)\n",
    "\n",
    "It also deals pretty well with newline (`\\n`) characters and other punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e20e2ced",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "s = '''Good muffins cost $3.88\\nin New York.  Please buy me\n",
    "... two of them.\\n\\nThanks.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc996a78",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$',\n",
       " '3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446f39a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### How does it work?\n",
    "\n",
    "Under the hood, `word_tokenize` uses the [`TreebankWordTokenizer`](https://www.nltk.org/api/nltk.tokenize.TreebankWordTokenizer.html#nltk.tokenize.TreebankWordTokenizer), which implements the following steps:\n",
    "\n",
    "- Split standard contractions (`\"don't\"`). \n",
    "- Treat most punctuation characters as separate tokens\n",
    "- Split off commas and single quotes, when followed by whitespace.\n",
    "- Separate periods that appear at the end of line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32fda83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tokenization in other languages\n",
    "\n",
    "- `nltk.word_tokenize` supports some other languages besides English, which can be entered using the `language` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39134c0d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "spanish_example = \"Después del almuerzo, Diego y Carlos fueron a la casa. Diego le dio las llaves a Carlos.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7a194fe",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Después', 'del', 'almuerzo', ',', 'Diego']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(spanish_example, language = 'spanish')[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3811d21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenizing *sentences*\n",
    "\n",
    "- Often, we are processing more than one sentence at a time.  \n",
    "- Before we tokenize words, it is useful to tokenize *sentences*.\n",
    "- `nltk` has a function called `sent_tokenize` that does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92d7267a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21feb941",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenizing *sentences*\n",
    "\n",
    "- Often, we are processing more than one sentence at a time.  \n",
    "- Before we tokenize words, it is useful to tokenize *sentences*.\n",
    "- `nltk` has a function called `sent_tokenize` that does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16d0428e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I visited my friend in Seattle.',\n",
       " 'We went to the park and then downtown.',\n",
       " 'It was a fun time.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"I visited my friend in Seattle. We went to the park and then downtown. It was a fun time.\"\n",
    "sent_tokenize(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6f6d7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stemming and lemmatizing\n",
    "\n",
    "> **Stemming** or **lemmatizing** a word means removing any *inflectional morphology* (e.g., plural markers) to reveal only the \"root\".\n",
    "\n",
    "- Very useful if you only have *root words* in a vocabulary set.  \n",
    "   - \"loved\" --> \"love\"\n",
    "   - \"running\" --> \"run\"\n",
    "- Stemming ≠ lemmatizing (the latter is a bit harder)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf20b32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stemming\n",
    "\n",
    "> **Stemming** is a process that removes common *affixes* from a word.\n",
    "\n",
    "- English examples: plural (\"-s\"), past tense (\"-ed\"), etc.  \n",
    "- Simple approach:\n",
    "   - Build list of common suffixes.  \n",
    "   - Identify these substrings in words.  \n",
    "   - Remove those substrings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe7c99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Stemming with `nltk`\n",
    "\n",
    "- The `PorterStemmer` implements the [**Porter Stemming Algorithm**](https://tartarus.org/martin/PorterStemmer/#:~:text=The%20Porter%20stemming%20algorithm%20(or,History).  \n",
    "- Imperfect process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "259fc6f0",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf0e3d8f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"dogs\") ### nice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2486855",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hous'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"houses\") ### hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b81e0e65",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fli'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"flies\") ### not so good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0109e553",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Stemming in action\n",
    "\n",
    "- Typically, you would first use `word_tokenize`, then **stem** those words. \n",
    "- E.g., useful for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c1aaa16",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'loved', 'that', 'film']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I loved that film\"\n",
    "words = word_tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1771a0b4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'that', 'film']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words = [ps.stem(w) for w in words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab854ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Check-in\n",
    "\n",
    "Try to come up with some examples where `ps.stem` works and where it doesn't work. What do you think about its performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f1caa37",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8af5652",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### A qualitative evaluation\n",
    "\n",
    "`ps.stem` works for prototypical examples, but also fails often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "964841d5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movi'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5abe5dae",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bottl'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"bottles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d050bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lemmatizing\n",
    "\n",
    "> **Lemmatizing** is a process for identifying the *root* or *lemma* of a word, which ensures that the output is indeed a root form.  \n",
    "\n",
    "- Stemming can be implemented with some simple rules (e.g., remove \"-s\").  \n",
    "  - Fast, but error-prone.\n",
    "- Lemmatizing requires a *list* of all the valid lemmas of a language. \n",
    "  - Slower and harder to build, but more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52433786",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Using `WordNetLemmatizer`\n",
    "\n",
    "- The `WordNetLemmatizer` uses the [**WordNet database**](https://www.nltk.org/howto/wordnet.html) as its dictionary of *lemmas*.  \n",
    "- Unlike stemming, lemmatizing should return a valid word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03ce9ab1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44afe22d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46159461",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bottle'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"bottles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5fd47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Check-in\n",
    "\n",
    "Try to come up with some more examples that `WordNetLemmatizer` does well on, but `PorterStemmer` does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5aaf377a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722517f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### More qualitative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af4f42cf",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "battl\n",
      "battle\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem(\"battles\"))\n",
    "print(lemmatizer.lemmatize(\"battles\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5707e606",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Specifying the `pos`\n",
    "\n",
    "- By specifying the `pos` tag, you can improve the accuracy of the lemmatization.\n",
    "- Noun (\"n\"), Verb (\"v\"), and adjective (\"a\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "438af17e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "better\n"
     ]
    }
   ],
   "source": [
    "### Without POS\n",
    "print(lemmatizer.lemmatize(\"running\"))  # Expected verb lemma \"run\", but defaults to noun\n",
    "print(lemmatizer.lemmatize(\"better\"))   # Expected adjective lemma \"good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf68f05e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "### Without POS\n",
    "print(lemmatizer.lemmatize(\"running\", pos = \"v\"))  # Expected verb lemma \"run\", but defaults to noun\n",
    "print(lemmatizer.lemmatize(\"better\", pos = \"a\"))   # Expected adjective lemma \"good\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defaccdc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### But how do you figure out part of speech?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8087d647",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part-of-speech tagging\n",
    "\n",
    "> **Part-of-speech** tagging is the process of identifying the part of speech of different words in a sentence (E.g., \"verb\", etc.).\n",
    "\n",
    "- The same *wordform* has different meanings depending on its **usage**.  \n",
    "- Can also be lemmatized differently (\"in the running\" vs. \"she is running\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98382461",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bdef25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `nltk.pos_tag`\n",
    "\n",
    "- Given a list of tokens...\n",
    "- ...`pos_tag` returns *tuples* of each word and its POS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d64a1c76",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "s1 = \"She is in the running for office.\"\n",
    "s2 = \"She is running for office.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b36804b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('She', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('running', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('office', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### First sentence\n",
    "pos_tag(word_tokenize(s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576b49a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Check-in\n",
    "\n",
    "Use `pos_tag` on the second sentence. What do you notice about the tag for \"running\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c1b2cf1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4627f6d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### A different POS\n",
    "\n",
    "Now, \"running\" is tagged as \"VBG\" (a \"gerund verb\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "766d6a18",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('She', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('running', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('office', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(word_tokenize(s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20532a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Processing web text\n",
    "\n",
    "> **Web scraping** is a common use case for CSS.  \n",
    "\n",
    "- We won't discuss the details too much here, but enough to get you started.  \n",
    "- You'll use `requests` to access text of web pages.\n",
    "- And then use `BeautifulSoup` and `nltk` to process that text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52291ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Necessary installations\n",
    "\n",
    "- If you don't have it already, you may need to install some of these libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8947349",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/seantrott/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: requests in /Users/seantrott/anaconda3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/seantrott/anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: click in /Users/seantrott/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/seantrott/anaconda3/lib/python3.11/site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/seantrott/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/seantrott/anaconda3/lib/python3.11/site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/seantrott/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/seantrott/anaconda3/lib/python3.11/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/seantrott/anaconda3/lib/python3.11/site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/seantrott/anaconda3/lib/python3.11/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/seantrott/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eedcc7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fetching web content with `requests`\n",
    "\n",
    "> The [`requests` library](https://pypi.org/project/requests/ is a library for retrieving the HTML from a web addres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46bf6b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests ### requests libriary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4a7c223",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "### Let's look at my CSS 2 syllabus page\n",
    "url = \"https://ucsd-css2.github.io/ucsd-css2-website/course/syllabus.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55ac04e2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch the content from the URL\n",
    "response = requests.get(url)\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1752fd5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### What's in the `Response`?\n",
    "\n",
    "- The `Response` object has a number of features, like `.json`, `.links`, and `.text`.  \n",
    "- We'll focus on `.text`, which is...all HTML!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80eca9cb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n<!DOCTYPE html>\\n\\n\\n<html lang=\"en\" data-content_root=\"\" >\\n\\n  <head>\\n    <meta charset=\"utf-8\" />\\n  '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde4ce94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simplifying with `BeautifulSoup`\n",
    "\n",
    "- `BeautifulSoup` can be used to **extract structure** from that HTML text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6eb95c8f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup ### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cea0bf04",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use BeautifulSoup to parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9706f0c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Getting the *text* from the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6f4cddc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CSS 2 Syllabus: Winter 2024 — CSS 2Skip to main contentBack to topCtrl+KWelcome to CSS 2!Course Logi'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the text from the web page\n",
    "text = soup.get_text(strip=True)\n",
    "text[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10fe0acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CSS', '2', 'Syllabus', ':', 'Winter', '2024', '—', 'CSS', '2Skip', 'to']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "tokens[0:10] ### not perfect, but not bad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba8d446",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Note on web APIs\n",
    "\n",
    "- Many websites have an **API**, or *Application Programming Interface*.  \n",
    "   - E.g., [Reddit API](https://praw.readthedocs.io/en/stable/.\n",
    "- This means you don't have to process all the web text yourself.  \n",
    "- Instead, you can work with more customized objects specific to the web page.  \n",
    "- But also often requires creating an **account**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be474201",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Web scraping and privacy use\n",
    "\n",
    "- Just because the text is *available*, doesn't mean it's *legal* or *ethical* to scrape it.  \n",
    "- Before scraping, think about our [privacy lectures](https://ucsd-css2.github.io/ucsd-css2-website/intro.html from CSS 2:\n",
    "   - Is scraping this content against the company's terms of service?  \n",
    "   - Would the people who produced this content be comfortable with me scraping it?  \n",
    "   - How do I plan to *use* that scraped content?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eade440",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture wrap-up\n",
    "\n",
    "- A big part of CSS is dealing with *unstructured* text data.  \n",
    "- `nltk` has methods to help us **extract structure**.  \n",
    "   - Tokenizing to find the words.  \n",
    "   - Stemming and lemmatizing to find the *roots* of those words.  \n",
    "- Other packages, like `BeautifulSoup`, can help simplify web text."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
